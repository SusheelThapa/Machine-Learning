{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMgzIQAcAlHMvFv+heYwW7N",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SusheelThapa/ML-From-Scratch/blob/tensorflow/tensorflow/tensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Fundamentals"
      ],
      "metadata": {
        "id": "vXmHX_AoHHFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction of Tensorflow"
      ],
      "metadata": {
        "id": "hFfVDzrKKWOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Tensorflow\n",
        "\n",
        "Use the below command, to install the ***tensorflow** in your local machine\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```"
      ],
      "metadata": {
        "id": "kSAEmyFcLcg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Tensorflow"
      ],
      "metadata": {
        "id": "XXv2op0hMHqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.version"
      ],
      "metadata": {
        "id": "UVd0AIxhIqkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is tensor?\n",
        "\n",
        "Tensor is a generalization of vectors and matrices to potentially higher dimension.\n",
        "\n",
        "Internally, tensorflow represent tensors as  n-dimensional arrays of base datatypes.\n",
        "\n",
        "Each tensor has a data type and a shape\n",
        "\n",
        "**Data Types** includes: float32, int32, string and others\n",
        "\n",
        "**Shape**: Represents the dimension of data"
      ],
      "metadata": {
        "id": "Qj9it6lkNBdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating tensor\n",
        "\n",
        "Below are the examples of creating tensor"
      ],
      "metadata": {
        "id": "_urOGnv5NIWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = tf.Variable(\"This is a string\", tf.string)\n",
        "number = tf.Variable(324, tf.int16)\n",
        "floating = tf.Variable(3.567,tf.float64)"
      ],
      "metadata": {
        "id": "YYzKbmsjND7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rank/Degree of Tensors\n",
        "\n",
        "Another word for rank is degree, it can be define as the number of dimensions involved in the tensor.\n",
        "\n",
        "In the above code block, what we have created is *tensor of rank zero*\n",
        "\n",
        "Now, let's create tensor of higher degree/ranks"
      ],
      "metadata": {
        "id": "keBvVsfINf-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank1_tensor = tf.Variable([\"Something\",\"Nothing\"], tf.string)"
      ],
      "metadata": {
        "id": "lG0c8vtKNco-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the rank of the tensor we can call `rank()` method as "
      ],
      "metadata": {
        "id": "_AtY_iYSOKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.rank(rank1_tensor)"
      ],
      "metadata": {
        "id": "ajlKBwOZOJp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shape of Tensors\n",
        "\n",
        "Shape of the tensors is simply the amount of elements that exist in each dimension.\n",
        "\n",
        "*Tensorflow will try to determine the shape of a tensor but sometimes it may be unknown*\n",
        "\n",
        "To get the shape of the tensor, we can call **shape attribute***"
      ],
      "metadata": {
        "id": "8uWPgHFdO__q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank1_tensor.shape"
      ],
      "metadata": {
        "id": "gnRm5RYoOZhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the shape\n",
        "\n",
        "Number of elements of a tensor is the product of the sizes of all its shape.\n",
        "\n",
        "Due to which many shapes that have the same number of elements, making it convient to be able to change the shape of a tensor\n",
        "\n",
        "Example of changing the shape of tensor"
      ],
      "metadata": {
        "id": "XFRf8Dg8Pt4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = tf.ones([1,2,3]) # tf.ones will create tensor of provide shape will all its element of ones\n",
        "\n",
        "tensor2 = tf.reshape(tensor1,[3,2,1]) # reshape the existing tensor to shape [3,2,1]\n",
        "\n",
        "tensor3= tf.reshape(tensor2,[3,-1]) # -1 tells tensor to calculate the size of the dimension at that place\n",
        "\n",
        "# The number of elements in orginal tensor and the reshape tensor is same"
      ],
      "metadata": {
        "id": "WhEKqca6PfhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets have a look at the shape of the tensor we have created"
      ],
      "metadata": {
        "id": "NVLkqzKqRO91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1.shape)\n",
        "print(tensor2.shape)\n",
        "print(tensor3.shape)"
      ],
      "metadata": {
        "id": "dH7HrQIxROTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of tensor\n",
        "\n",
        "Commonly used tensor are as follows:\n",
        "- Variable\n",
        "- Constant\n",
        "- Placeholder\n",
        "- SparseTensor"
      ],
      "metadata": {
        "id": "h_k8EN-9ReR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Learning Algorithms\n",
        "\n",
        "We will be studying 4 fundamental machine learning algorithms.\n",
        "\n",
        "- Linear Regression\n",
        "- Classification\n",
        "- Clustering\n",
        "- Hidden Markov Models\n"
      ],
      "metadata": {
        "id": "Js7sza0dWmXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). (***Wikipedia***)"
      ],
      "metadata": {
        "id": "uFXk6GTlXRpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup and Imports"
      ],
      "metadata": {
        "id": "5oGWz7X_YE_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np # Optimize version of array\n",
        "import pandas as pd # Data analytics tools\n",
        "import matplotlib.pyplot as plt # Visualization tools\n",
        "\n",
        "import IPython.display as clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc # Required later in linear regression\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "NQPdzKz8Wxvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data\n",
        "\n",
        "The dataset we will be focusing here will be titanic dataset. It has tons of information about each passanger on the ship.\n",
        "\n",
        "**Below, we will load a dataset and learn how we can explore it using some built-in tools**"
      ],
      "metadata": {
        "id": "rByhX4GVZYZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "\n",
        "# Training datasets\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') \n",
        "y_train = dftrain.pop('survived')\n",
        "\n",
        "# Testing datasets\n",
        "dftest = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # Testing datasets\n",
        "y_test = dftest.pop('survived')"
      ],
      "metadata": {
        "id": "qeP_g3AZYi9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pd.read_csv()` method will return a new pandas *dataframe*. Dataframe is like a table and actually have a look at the table representation.\n",
        "\n",
        "We have decided to pop the \"survived\" column from our dataset and store it in a new varible as this column tells us whether the passanger survived or not. It is most like to be something that our model should predict\n",
        "\n",
        "To look at the data we will use `head()` method from pandas."
      ],
      "metadata": {
        "id": "m58q78WMaak7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.head()"
      ],
      "metadata": {
        "id": "i4rinQLQZvA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we need more statical description of the data we can use `describe()` method"
      ],
      "metadata": {
        "id": "HoltfuBkcOj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.describe()"
      ],
      "metadata": {
        "id": "1d95wBQZbMES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the information about the dataype of each column, number of columns and what are those we can use `info()` method of pandas"
      ],
      "metadata": {
        "id": "Ec-wPrmHcnqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.info()"
      ],
      "metadata": {
        "id": "Wn5BhkLgceTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the shape of the dataframe"
      ],
      "metadata": {
        "id": "NA8l_30xdFYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.shape"
      ],
      "metadata": {
        "id": "ibSwsp_-cmQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the data we have got."
      ],
      "metadata": {
        "id": "SD2gG2zZdZkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.age.hist(bins=20)"
      ],
      "metadata": {
        "id": "q3vCndn5dL5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.sex.value_counts().plot(kind='barh')"
      ],
      "metadata": {
        "id": "7tumauZzdgDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain['class'].value_counts().plot(kind='barh')"
      ],
      "metadata": {
        "id": "h06xPqUtdmrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([dftrain,y_train],axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survived')"
      ],
      "metadata": {
        "id": "5JLOOC3jueKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing this information we should notice the following:\n",
        "- The majority of passangers are in their 20's or 30's\n",
        "- The majority of passengers are male\n",
        "- The majority of passengers are in \"Third Class\"\n",
        "- Females have a much higher chances of survival"
      ],
      "metadata": {
        "id": "8_yLgfv2d__B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training vs Testing Data\n",
        "\n",
        "**Training Data** is what we feed to the model so that it can develop and learn. It is usually much larger size than the testing data\n",
        "\n",
        "**Testing Data** is what we use to evaluate the model and see how well it is performing. It is important to use seperate set of data that the model has not been trained on to evaluate it.\n"
      ],
      "metadata": {
        "id": "fa16QYvdetm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features Columns\n",
        "\n",
        "In the dataset, we have two types of information\n",
        "- **Categorical**\n",
        "\n",
        "    It is anything that isn't numerical.\n",
        "\n",
        "    *For example, the sex column does use numbers, it use words 'male' and \"female\"*\n",
        "\n",
        "- **Numeric**\n",
        "\n",
        "    These are the data with numeric value.\n",
        "\n",
        "Before continuing, we need to change all our categorical data into numeric data.\n",
        "Todo this, Tensorflow has some tools to help us."
      ],
      "metadata": {
        "id": "Ub4n1pT2lq3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses','parch','class','deck','embark_town','alone']\n",
        "NUMERIC_COLUMNS = ['age','fare']\n",
        "\n",
        "feature_columns =[]\n",
        "\n",
        "for feature in CATEGORICAL_COLUMNS:\n",
        "    vocabulary = dftrain[feature].unique()\n",
        "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature,vocabulary))\n",
        "\n",
        "\n",
        "for feature in NUMERIC_COLUMNS:\n",
        "    feature_columns.append(tf.feature_column.numeric_column(feature,dtype=tf.float32))\n"
      ],
      "metadata": {
        "id": "mfHMpFK1dsCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model\n",
        "\n",
        "Training the model describes about how the model is being train. Specifically speaking how data is fed to our model.\n",
        "\n",
        "To train the model, we will fed the model with data of batch size of 32. It means we will fed small batches of entries to our model multiple times according to the **epoches**\n",
        "\n",
        "**Epoches** is one stream of our entire datasets. Number of epoches we define is the amount of times our model will see the entire dataset.\n",
        "\n",
        "*Examples: If we have 10 epocs, our model will see the same datasets 10 times.*\n",
        "\n",
        "To feed our data to model in the form of batches we need ***input function*** which task is to convert our dataset into batches at each epoch"
      ],
      "metadata": {
        "id": "2R_RzjXIvrK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Input function\n",
        "\n",
        "The Tensorflow model we are going to use requires that the data we pass it comes in as `tf.data.Dataset` object.\n",
        "\n",
        "It means that we must create a *input function* that can convert our current pandas dataframe into that object.\n",
        "\n",
        "*input_function* show below is directly copied from tensorflow documentation."
      ],
      "metadata": {
        "id": "lNv3gy-vxHTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) # Create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000) # randomize the order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs) # split dataset into batches of 32 and repeat the process for number of epochs\n",
        "    return ds # return a batch of the dataset\n",
        "  return input_function # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)\n",
        "eval_input_fn = make_input_fn(dftrain, y_train, num_epochs=1, shuffle=False)"
      ],
      "metadata": {
        "id": "RIQp6anExEL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating the model\n",
        "\n",
        "We will be using linear estimator to utilize the linear regression algorithm."
      ],
      "metadata": {
        "id": "-9ubaazCzlOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns) # We are creating a linear estimator by passing the feature columns we created earlier."
      ],
      "metadata": {
        "id": "h89lu4Diz0MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training the model\n",
        "\n",
        "Training the model is as easy as passing the input functions that we created earlier."
      ],
      "metadata": {
        "id": "SmWbzVNV0jGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "linear_est.train(train_input_fn) # just passing the input function"
      ],
      "metadata": {
        "id": "aL_MWepPz8if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing our model\n",
        "Testing is also same as training the model but here we will be passing input function for testing dataset"
      ],
      "metadata": {
        "id": "0XKkMNxD1Lc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = linear_est.evaluate(eval_input_fn)\n",
        "\n",
        "print(\"The accuracy of our model is \",result['accuracy'])"
      ],
      "metadata": {
        "id": "CWggySSZ1XAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting using our model"
      ],
      "metadata": {
        "id": "CMYdP9Cl2uVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = list(linear_est.predict(eval_input_fn))\n",
        "\n",
        "print(\"Passanger chance of survival is \",result[100]['probabilities'][1])"
      ],
      "metadata": {
        "id": "-fV0HcKG2wj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "hVDXxsS3ATy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing the necessary packages"
      ],
      "metadata": {
        "id": "RptsNiM7BuJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WBpn50ubB0HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasets\n",
        "\n",
        "This species dataset seperates the flower into 3 different classes of species\n",
        "- Setosa\n",
        "- Versicolor\n",
        "- Virginica\n",
        "\n",
        "The information about each flower is the following:\n",
        "- sepal length\n",
        "- sepal width\n",
        "- petal length\n",
        "- petal width\n"
      ],
      "metadata": {
        "id": "a8Cq2Gv5BFta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the datasets\n",
        "\n",
        "Next, we will be loading the datasets"
      ],
      "metadata": {
        "id": "nod3RD7RBn32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some constant that will help later on\n",
        "CSV_COLUMN_NAMES = [\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\",\"Species\"]\n",
        "SPECIES = [\"Setosa\",\"Versicolor\",\"Virginica\"]\n",
        "\n",
        "# Loading the datasets, we are using keras to grab our datasets and read them into pandas dataframe\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
        ")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
        ")\n",
        "\n",
        "train = pd.read_csv(train_path,names= CSV_COLUMN_NAMES,header =0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
      ],
      "metadata": {
        "id": "688qXySJAWcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our datasets."
      ],
      "metadata": {
        "id": "DsNEN21tDTLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "e3tusWKjCqye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can pop the \"Species\" as they are label to classify."
      ],
      "metadata": {
        "id": "4-HW2xH9DZB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train.pop('Species')\n",
        "y_test = test.pop('Species')"
      ],
      "metadata": {
        "id": "eXQLXQfRC1QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into shape of our datasets"
      ],
      "metadata": {
        "id": "DYf4Du8XEL6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "w3L8YA7YDjB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have 120 data with 4 features"
      ],
      "metadata": {
        "id": "eDCllXq3ETa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input function"
      ],
      "metadata": {
        "id": "_FZoaDwTEekr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features),labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode\n",
        "    if training:\n",
        "        dataset= dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "mXDuwhHnEgto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features columns"
      ],
      "metadata": {
        "id": "N8i_tlQoFQWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_feature_columns = []\n",
        "\n",
        "for key in train.keys():\n",
        "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
      ],
      "metadata": {
        "id": "dammCcgPFNRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the model\n",
        "\n",
        "For classification tasks there are variety of different esitmators/models that we can pick from.\n",
        "\n",
        "Some options are listed below:\n",
        "\n",
        "- `DNNClassifier`(Deep Neural Network)\n",
        "- `LinearClassifier`\n",
        "\n",
        "We can choose either model but DNN is the best choice as we may not be able to  find a linear correspondence in our data."
      ],
      "metadata": {
        "id": "YoBayCnyFs9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a DNN with 2 hidden layer with 30 and 10 hidden nodes each\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns = my_feature_columns,\n",
        "    hidden_units=[30,10], # Defining the two hidden layer\n",
        "    n_classes =3 #model must chose between 3 classes\n",
        ")"
      ],
      "metadata": {
        "id": "SCJyOyztFiGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model"
      ],
      "metadata": {
        "id": "mRiM8HT3HSVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.train(\n",
        "    input_fn = lambda: input_fn(train,y_train, training=True),\n",
        "    steps=5000\n",
        ")"
      ],
      "metadata": {
        "id": "U63pOGwtHVvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the model"
      ],
      "metadata": {
        "id": "5KM7dfq7IJHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.evaluate(\n",
        "    input_fn=lambda:input_fn(test, y_test, training = False)\n",
        "    )"
      ],
      "metadata": {
        "id": "bAYEgc0EILbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making prediction"
      ],
      "metadata": {
        "id": "YIFlosljI2BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn_pred(features, batch_size=256):\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print(\"Please type numeric values as prompted.\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid: \n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn_pred(predict))\n",
        "for pred_dict in predictions:\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))\n"
      ],
      "metadata": {
        "id": "9SCEHkktIfbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clustering\n",
        "\n",
        "Clustering is a machine learning technique that involves the grouping of data points. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features."
      ],
      "metadata": {
        "id": "LUB2XE4wLV6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Algorithm for K-means clustering\n",
        "\n",
        "- Randomly pick K points to place K centroids\n",
        "- Assign all of the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n",
        "- Average all of the points belonging to each centroid to find the middle of those clusters(center of mass). Place the corresponding centroids into that position\n",
        "- Reassign every point once again to the closest centroid\n",
        "- Repeat steps 3-4 until no points changes which centroid it belongs to"
      ],
      "metadata": {
        "id": "YM_4vcAgP4kX"
      }
    }
  ]
}