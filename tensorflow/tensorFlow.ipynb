{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPJG1K9HKemX+ffrGNfa8zY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SusheelThapa/ML-From-Scratch/blob/tensorflow/tensorflow/tensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Fundamentals"
      ],
      "metadata": {
        "id": "vXmHX_AoHHFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction of Tensorflow"
      ],
      "metadata": {
        "id": "hFfVDzrKKWOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Tensorflow\n",
        "\n",
        "Use the below command, to install the ***tensorflow** in your local machine\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```"
      ],
      "metadata": {
        "id": "kSAEmyFcLcg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Tensorflow"
      ],
      "metadata": {
        "id": "XXv2op0hMHqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.version"
      ],
      "metadata": {
        "id": "UVd0AIxhIqkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is tensor?\n",
        "\n",
        "Tensor is a generalization of vectors and matrices to potentially higher dimension.\n",
        "\n",
        "Internally, tensorflow represent tensors as  n-dimensional arrays of base datatypes.\n",
        "\n",
        "Each tensor has a data type and a shape\n",
        "\n",
        "**Data Types** includes: float32, int32, string and others\n",
        "\n",
        "**Shape**: Represents the dimension of data"
      ],
      "metadata": {
        "id": "Qj9it6lkNBdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating tensor\n",
        "\n",
        "Below are the examples of creating tensor"
      ],
      "metadata": {
        "id": "_urOGnv5NIWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = tf.Variable(\"This is a string\", tf.string)\n",
        "number = tf.Variable(324, tf.int16)\n",
        "floating = tf.Variable(3.567,tf.float64)"
      ],
      "metadata": {
        "id": "YYzKbmsjND7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rank/Degree of Tensors\n",
        "\n",
        "Another word for rank is degree, it can be define as the number of dimensions involved in the tensor.\n",
        "\n",
        "In the above code block, what we have created is *tensor of rank zero*\n",
        "\n",
        "Now, let's create tensor of higher degree/ranks"
      ],
      "metadata": {
        "id": "keBvVsfINf-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank1_tensor = tf.Variable([\"Something\",\"Nothing\"], tf.string)"
      ],
      "metadata": {
        "id": "lG0c8vtKNco-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the rank of the tensor we can call `rank()` method as "
      ],
      "metadata": {
        "id": "_AtY_iYSOKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.rank(rank1_tensor)"
      ],
      "metadata": {
        "id": "ajlKBwOZOJp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shape of Tensors\n",
        "\n",
        "Shape of the tensors is simply the amount of elements that exist in each dimension.\n",
        "\n",
        "*Tensorflow will try to determine the shape of a tensor but sometimes it may be unknown*\n",
        "\n",
        "To get the shape of the tensor, we can call **shape attribute***"
      ],
      "metadata": {
        "id": "8uWPgHFdO__q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank1_tensor.shape"
      ],
      "metadata": {
        "id": "gnRm5RYoOZhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the shape\n",
        "\n",
        "Number of elements of a tensor is the product of the sizes of all its shape.\n",
        "\n",
        "Due to which many shapes that have the same number of elements, making it convient to be able to change the shape of a tensor\n",
        "\n",
        "Example of changing the shape of tensor"
      ],
      "metadata": {
        "id": "XFRf8Dg8Pt4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = tf.ones([1,2,3]) # tf.ones will create tensor of provide shape will all its element of ones\n",
        "\n",
        "tensor2 = tf.reshape(tensor1,[3,2,1]) # reshape the existing tensor to shape [3,2,1]\n",
        "\n",
        "tensor3= tf.reshape(tensor2,[3,-1]) # -1 tells tensor to calculate the size of the dimension at that place\n",
        "\n",
        "# The number of elements in orginal tensor and the reshape tensor is same"
      ],
      "metadata": {
        "id": "WhEKqca6PfhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets have a look at the shape of the tensor we have created"
      ],
      "metadata": {
        "id": "NVLkqzKqRO91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1.shape)\n",
        "print(tensor2.shape)\n",
        "print(tensor3.shape)"
      ],
      "metadata": {
        "id": "dH7HrQIxROTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of tensor\n",
        "\n",
        "Commonly used tensor are as follows:\n",
        "- Variable\n",
        "- Constant\n",
        "- Placeholder\n",
        "- SparseTensor"
      ],
      "metadata": {
        "id": "h_k8EN-9ReR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Learning Algorithms\n",
        "\n",
        "We will be studying 4 fundamental machine learning algorithms.\n",
        "\n",
        "- Linear Regression\n",
        "- Classification\n",
        "- Clustering\n",
        "- Hidden Markov Models\n"
      ],
      "metadata": {
        "id": "Js7sza0dWmXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). (***Wikipedia***)"
      ],
      "metadata": {
        "id": "uFXk6GTlXRpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup and Imports"
      ],
      "metadata": {
        "id": "5oGWz7X_YE_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np # Optimize version of array\n",
        "import pandas as pd # Data analytics tools\n",
        "import matplotlib.pyplot as plt # Visualization tools\n",
        "\n",
        "import IPython.display as clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc # Required later in linear regression\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "NQPdzKz8Wxvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data\n",
        "\n",
        "The dataset we will be focusing here will be titanic dataset. It has tons of information about each passanger on the ship.\n",
        "\n",
        "**Below, we will load a dataset and learn how we can explore it using some built-in tools**"
      ],
      "metadata": {
        "id": "rByhX4GVZYZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "\n",
        "# Training datasets\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') \n",
        "y_train = dftrain.pop('survived')\n",
        "\n",
        "# Testing datasets\n",
        "dftest = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # Testing datasets\n",
        "y_test = dftest.pop('survived')"
      ],
      "metadata": {
        "id": "qeP_g3AZYi9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pd.read_csv()` method will return a new pandas *dataframe*. Dataframe is like a table and actually have a look at the table representation.\n",
        "\n",
        "We have decided to pop the \"survived\" column from our dataset and store it in a new varible as this column tells us whether the passanger survived or not. It is most like to be something that our model should predict\n",
        "\n",
        "To look at the data we will use `head()` method from pandas."
      ],
      "metadata": {
        "id": "m58q78WMaak7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.head()"
      ],
      "metadata": {
        "id": "i4rinQLQZvA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we need more statical description of the data we can use `describe()` method"
      ],
      "metadata": {
        "id": "HoltfuBkcOj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.describe()"
      ],
      "metadata": {
        "id": "1d95wBQZbMES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the information about the dataype of each column, number of columns and what are those we can use `info()` method of pandas"
      ],
      "metadata": {
        "id": "Ec-wPrmHcnqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.info()"
      ],
      "metadata": {
        "id": "Wn5BhkLgceTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the shape of the dataframe"
      ],
      "metadata": {
        "id": "NA8l_30xdFYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.shape"
      ],
      "metadata": {
        "id": "ibSwsp_-cmQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the data we have got."
      ],
      "metadata": {
        "id": "SD2gG2zZdZkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.age.hist(bins=20)"
      ],
      "metadata": {
        "id": "q3vCndn5dL5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.sex.value_counts().plot(kind='barh')"
      ],
      "metadata": {
        "id": "7tumauZzdgDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain['class'].value_counts().plot(kind='barh')"
      ],
      "metadata": {
        "id": "h06xPqUtdmrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([dftrain,y_train],axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survived')"
      ],
      "metadata": {
        "id": "5JLOOC3jueKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing this information we should notice the following:\n",
        "- The majority of passangers are in their 20's or 30's\n",
        "- The majority of passengers are male\n",
        "- The majority of passengers are in \"Third Class\"\n",
        "- Females have a much higher chances of survival"
      ],
      "metadata": {
        "id": "8_yLgfv2d__B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training vs Testing Data\n",
        "\n",
        "**Training Data** is what we feed to the model so that it can develop and learn. It is usually much larger size than the testing data\n",
        "\n",
        "**Testing Data** is what we use to evaluate the model and see how well it is performing. It is important to use seperate set of data that the model has not been trained on to evaluate it.\n"
      ],
      "metadata": {
        "id": "fa16QYvdetm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features Columns\n",
        "\n",
        "In the dataset, we have two types of information\n",
        "- **Categorical**\n",
        "\n",
        "    It is anything that isn't numerical.\n",
        "\n",
        "    *For example, the sex column does use numbers, it use words 'male' and \"female\"*\n",
        "\n",
        "- **Numeric**\n",
        "\n",
        "    These are the data with numeric value.\n",
        "\n",
        "Before continuing, we need to change all our categorical data into numeric data.\n",
        "Todo this, Tensorflow has some tools to help us."
      ],
      "metadata": {
        "id": "Ub4n1pT2lq3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses','parch','class','deck','embark_town','alone']\n",
        "NUMERIC_COLUMNS = ['age','fare']\n",
        "\n",
        "feature_columns =[]\n",
        "\n",
        "for feature in CATEGORICAL_COLUMNS:\n",
        "    vocabulary = dftrain[feature].unique()\n",
        "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature,vocabulary))\n",
        "\n",
        "\n",
        "for feature in NUMERIC_COLUMNS:\n",
        "    feature_columns.append(tf.feature_column.numeric_column(feature,dtype=tf.float32))\n"
      ],
      "metadata": {
        "id": "mfHMpFK1dsCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model\n",
        "\n",
        "Training the model describes about how the model is being train. Specifically speaking how data is fed to our model.\n",
        "\n",
        "To train the model, we will fed the model with data of batch size of 32. It means we will fed small batches of entries to our model multiple times according to the **epoches**\n",
        "\n",
        "**Epoches** is one stream of our entire datasets. Number of epoches we define is the amount of times our model will see the entire dataset.\n",
        "\n",
        "*Examples: If we have 10 epocs, our model will see the same datasets 10 times.*\n",
        "\n",
        "To feed our data to model in the form of batches we need ***input function*** which task is to convert our dataset into batches at each epoch"
      ],
      "metadata": {
        "id": "2R_RzjXIvrK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Input function\n",
        "\n",
        "The Tensorflow model we are going to use requires that the data we pass it comes in as `tf.data.Dataset` object.\n",
        "\n",
        "It means that we must create a *input function* that can convert our current pandas dataframe into that object.\n",
        "\n",
        "*input_function* show below is directly copied from tensorflow documentation."
      ],
      "metadata": {
        "id": "lNv3gy-vxHTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) # Create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000) # randomize the order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs) # split dataset into batches of 32 and repeat the process for number of epochs\n",
        "    return ds # return a batch of the dataset\n",
        "  return input_function # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)\n",
        "eval_input_fn = make_input_fn(dftrain, y_train, num_epochs=1, shuffle=False)"
      ],
      "metadata": {
        "id": "RIQp6anExEL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating the model\n",
        "\n",
        "We will be using linear estimator to utilize the linear regression algorithm."
      ],
      "metadata": {
        "id": "-9ubaazCzlOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns) # We are creating a linear estimator by passing the feature columns we created earlier."
      ],
      "metadata": {
        "id": "h89lu4Diz0MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training the model\n",
        "\n",
        "Training the model is as easy as passing the input functions that we created earlier."
      ],
      "metadata": {
        "id": "SmWbzVNV0jGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "linear_est.train(train_input_fn) # just passing the input function"
      ],
      "metadata": {
        "id": "aL_MWepPz8if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing our model\n",
        "Testing is also same as training the model but here we will be passing input function for testing dataset"
      ],
      "metadata": {
        "id": "0XKkMNxD1Lc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = linear_est.evaluate(eval_input_fn)\n",
        "\n",
        "print(\"The accuracy of our model is \",result['accuracy'])"
      ],
      "metadata": {
        "id": "CWggySSZ1XAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting using our model"
      ],
      "metadata": {
        "id": "CMYdP9Cl2uVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = list(linear_est.predict(eval_input_fn))\n",
        "\n",
        "print(\"Passanger chance of survival is \",result[100]['probabilities'][1])"
      ],
      "metadata": {
        "id": "-fV0HcKG2wj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "hVDXxsS3ATy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing the necessary packages"
      ],
      "metadata": {
        "id": "RptsNiM7BuJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WBpn50ubB0HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasets\n",
        "\n",
        "This species dataset seperates the flower into 3 different classes of species\n",
        "- Setosa\n",
        "- Versicolor\n",
        "- Virginica\n",
        "\n",
        "The information about each flower is the following:\n",
        "- sepal length\n",
        "- sepal width\n",
        "- petal length\n",
        "- petal width\n"
      ],
      "metadata": {
        "id": "a8Cq2Gv5BFta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the datasets\n",
        "\n",
        "Next, we will be loading the datasets"
      ],
      "metadata": {
        "id": "nod3RD7RBn32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some constant that will help later on\n",
        "CSV_COLUMN_NAMES = [\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\",\"Species\"]\n",
        "SPECIES = [\"Setosa\",\"Versicolor\",\"Virginica\"]\n",
        "\n",
        "# Loading the datasets, we are using keras to grab our datasets and read them into pandas dataframe\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
        ")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
        ")\n",
        "\n",
        "train = pd.read_csv(train_path,names= CSV_COLUMN_NAMES,header =0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
      ],
      "metadata": {
        "id": "688qXySJAWcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our datasets."
      ],
      "metadata": {
        "id": "DsNEN21tDTLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "e3tusWKjCqye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can pop the \"Species\" as they are label to classify."
      ],
      "metadata": {
        "id": "4-HW2xH9DZB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train.pop('Species')\n",
        "y_test = test.pop('Species')"
      ],
      "metadata": {
        "id": "eXQLXQfRC1QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into shape of our datasets"
      ],
      "metadata": {
        "id": "DYf4Du8XEL6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "w3L8YA7YDjB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have 120 data with 4 features"
      ],
      "metadata": {
        "id": "eDCllXq3ETa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input function"
      ],
      "metadata": {
        "id": "_FZoaDwTEekr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features),labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode\n",
        "    if training:\n",
        "        dataset= dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "mXDuwhHnEgto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features columns"
      ],
      "metadata": {
        "id": "N8i_tlQoFQWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_feature_columns = []\n",
        "\n",
        "for key in train.keys():\n",
        "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
      ],
      "metadata": {
        "id": "dammCcgPFNRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the model\n",
        "\n",
        "For classification tasks there are variety of different esitmators/models that we can pick from.\n",
        "\n",
        "Some options are listed below:\n",
        "\n",
        "- `DNNClassifier`(Deep Neural Network)\n",
        "- `LinearClassifier`\n",
        "\n",
        "We can choose either model but DNN is the best choice as we may not be able to  find a linear correspondence in our data."
      ],
      "metadata": {
        "id": "YoBayCnyFs9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a DNN with 2 hidden layer with 30 and 10 hidden nodes each\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns = my_feature_columns,\n",
        "    hidden_units=[30,10], # Defining the two hidden layer\n",
        "    n_classes =3 #model must chose between 3 classes\n",
        ")"
      ],
      "metadata": {
        "id": "SCJyOyztFiGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model"
      ],
      "metadata": {
        "id": "mRiM8HT3HSVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.train(\n",
        "    input_fn = lambda: input_fn(train,y_train, training=True),\n",
        "    steps=5000\n",
        ")"
      ],
      "metadata": {
        "id": "U63pOGwtHVvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the model"
      ],
      "metadata": {
        "id": "5KM7dfq7IJHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.evaluate(\n",
        "    input_fn=lambda:input_fn(test, y_test, training = False)\n",
        "    )"
      ],
      "metadata": {
        "id": "bAYEgc0EILbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making prediction"
      ],
      "metadata": {
        "id": "YIFlosljI2BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn_pred(features, batch_size=256):\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print(\"Please type numeric values as prompted.\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid: \n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn_pred(predict))\n",
        "for pred_dict in predictions:\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))\n"
      ],
      "metadata": {
        "id": "9SCEHkktIfbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering\n",
        "\n",
        "Clustering is a machine learning technique that involves the grouping of data points. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features."
      ],
      "metadata": {
        "id": "LUB2XE4wLV6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algorithm for K-means clustering\n",
        "\n",
        "- Randomly pick K points to place K centroids\n",
        "- Assign all of the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n",
        "- Average all of the points belonging to each centroid to find the middle of those clusters(center of mass). Place the corresponding centroids into that position\n",
        "- Reassign every point once again to the closest centroid\n",
        "- Repeat steps 3-4 until no points changes which centroid it belongs to"
      ],
      "metadata": {
        "id": "YM_4vcAgP4kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hidden Markov Models\n",
        "\n",
        "Hidden Markov Models is a finite sets of state, each of which is associated with a(generally multidimensional) probality distribution.\n",
        "\n",
        "Transition among the states are governed by a set of **probalities** called transition probalities.\n",
        "\n",
        "A hidden markov models works with probalities to predict future events or states.\n",
        "\n",
        "We will be creating a hidden markov model that can predict the weather.\n"
      ],
      "metadata": {
        "id": "lRTdoF8gSgB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data in Hidden Markov Models\n",
        "\n",
        "**States**\n",
        "\n",
        "In each markov model we have a finite set of states. These states could be something like \"warm\" and \"cold\" or \"high\" and \"low\".\n",
        "\n",
        "These states are \"hidden\" within the model, which mean we do not directly observe them.\n",
        "\n",
        "**Observations**\n",
        "\n",
        "Each state had a particular outcome or observation assocaited with it based on probality distribution.\n",
        "\n",
        "*Examples: On a hot day, Tim has a 80% chance of being happy and a 20% chance of being sad.*\n",
        "\n",
        "**Transitions**\n",
        "\n",
        "Each state will have a probability defining the likelyhood of transforming to a different state. \n",
        "\n",
        "*Example: A cold day has a 30% chance of being followed by a hot day and a 70% chance of being followed by another cold day*\n",
        "\n",
        "To create a hidden markov model we need:\n",
        "- States\n",
        "- Observation Distribution\n",
        "- Transition Distribution\n",
        "\n",
        "For our purpose, we will assume we already have this information avaliable as we attempt to predict the weather on a given day.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYHiTnDLTTKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "\n",
        "An artificial neural network learning algorithm, or neural network, or just neural network, is a computational learning system that uses a network of functions to understand and translate a data input of one form into a desired output, usually in another form. The concept of the artificial neural network was inspired by human biology and the way neurons of the human brain function together to understand inputs from human senses. *(https://deepai.org/machine-learning-glossary-and-terms/neural-network)*"
      ],
      "metadata": {
        "id": "kvx_tNQk61_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "-6U2mPWQ-rPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "biV8oeW964HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets\n",
        "\n",
        "We will be using MNIST Fashain Dataset. This dataset is included in keras and it includes 60000 images for training and 10,000 images for validation/testing."
      ],
      "metadata": {
        "id": "fJjsOELN-4qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "# Spliting into training and testing datasets\n",
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "D5hpTap3-33b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look as it shape"
      ],
      "metadata": {
        "id": "C1gTcA33_hnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "id": "_iLMHc-Q_QuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It means that we have got 60000 images of 28 * 28 pixels for training the models"
      ],
      "metadata": {
        "id": "kcLG3weL_t9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0,23,23]"
      ],
      "metadata": {
        "id": "gERIwBys_n9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pixel values are between 0 and 255, 0 being black and 255 being white. It means that we have grayscale images as there are no color other than black and white."
      ],
      "metadata": {
        "id": "NCqcOioMATGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[:10]"
      ],
      "metadata": {
        "id": "Y98hSZJzAEKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our label ranges from 0-9. Each integer represents a specific article of clothing."
      ],
      "metadata": {
        "id": "70kXYB5CBI3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "metadata": {
        "id": "DHULkAH4Akc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally lets have a look at what some of these images look like"
      ],
      "metadata": {
        "id": "3TSAt_rcBV9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[1])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_dNr2_TKBUlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Data preprocessing means applying some prior transformation to our data before feeding to the model. In this case, we will simply scale all of our greyscale pixel values(0-255) to be between 0 and 1.\n",
        "\n",
        "To do this, we will divided training and testing sets by 255.0. We do this because smaller values will make it easier for the model to process our values."
      ],
      "metadata": {
        "id": "vEwRHVE-BokN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images =  train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "7FWlXdVpBquF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model\n",
        "\n",
        "We are going to use keras *sequential* model with three different layer. This model represents a feed-forward neural network(one that passes values from left to right)."
      ],
      "metadata": {
        "id": "0cMO-mfrCj4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)), # input layer (1)\n",
        "    keras.layers.Dense(128,activation='relu'), # hidden layer (2)\n",
        "    keras.layers.Dense(10,activation='softmax')# output layer (3)\n",
        "])"
      ],
      "metadata": {
        "id": "DMSBcUnYCZIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer 1**\n",
        "\n",
        "This is our input layer and it will consist fo 784 neurons. We use the flatten layer with an input shape of (28,28) to denote that our input should come in that shape. The flatten means that our layer will reshape the shape (28,28) array into a vector of 784 neurons so that each pixel will be associated with one neuron.\n",
        "\n",
        "**Layer 2**\n",
        "\n",
        "This is our first and only hidden layer. The dense denotes that this layer will be fully connected and each neyron from the previous layer connects to each neuron of this layer. It has 128 neurons and uses the rectify linear unit activation function.\n",
        "\n",
        "**Layer 3**\n",
        "\n",
        "This is our output layer and is also a dense layer. It has 10 neurons that we will look at to determine our models output. Each neuron represents the probabillity of a given images being one of the 10 different classes. The activation function *softmax* is used on this layer to calculate a probabillity distribution for each class. This means the value of any neuron in this layer will be between 0 and 1, where 1 represents a high probability of the image being that class."
      ],
      "metadata": {
        "id": "xgeP3nk9EN7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compile the Model\n",
        "\n",
        "This is the last step of building the model. While compiling the model we will specify the loss function, optimizer adn metrics we would like to track.\n",
        "\n",
        "We will be using ***sparse_categorical_crossentropy*** as loss function and ***adam*** as optimizer."
      ],
      "metadata": {
        "id": "vC5kP0EkFwFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DYRCZdBFFt6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "Now that we have build the model, we will be training the model with the preprocessed data."
      ],
      "metadata": {
        "id": "szU6HjYZGk3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images,train_labels,epochs=5)"
      ],
      "metadata": {
        "id": "TE52gQmjGZkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model\n",
        "\n",
        "After we have train the model, we will be looking into evaluating the model we have created.\n",
        "\n",
        "The *verbose* argument can have two values 0 or 1 where 0 = slient and 1 = progress bar."
      ],
      "metadata": {
        "id": "5hqAgCnMHe4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=1)\n",
        "\n",
        "print(\"Test accuracy = \", test_acc)"
      ],
      "metadata": {
        "id": "JTB5B6_cHnP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions\n",
        "\n",
        "To make predictions we simply need to pass an array of data in the form we've specified in the input layer to `.predict()` method."
      ],
      "metadata": {
        "id": "q3boEMc1IlRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "metadata": {
        "id": "BQAzhI9sH-bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THis method returns to us an array of predictions for each images we passed to it. Let's have a look at the predictions for image 1."
      ],
      "metadata": {
        "id": "ug38jIU-JS_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0]"
      ],
      "metadata": {
        "id": "tV_LaSPPI0Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to get the value withthe highest score we can use `argmax()` function from numpy. This simply returns hte index of the maximum value from an numpy array."
      ],
      "metadata": {
        "id": "uLd_qyWFKraY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_index = np.argmax(predictions[0])\n",
        "print(\"Prediction is \", class_names[test_labels[label_index]])"
      ],
      "metadata": {
        "id": "jNudG5XNJLXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check if the prediction is correct or not"
      ],
      "metadata": {
        "id": "TL4io9HKMcgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_images[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GnrgdYr3MGkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Computer Vision\n",
        "\n",
        "We will learn how to perform **image classification** and **object detection/recognition** using *deep computer vision* with something called ***convolutional neural network***\n",
        "\n",
        "The goal of the covolution neural network will be to classify and detect images or specific objects from within the images. We will be using image data as our feature and a label for those images as our label or output.\n",
        "\n",
        "Apart from the basis of Neural Network we will look into following terms:\n",
        "- Image Data\n",
        "- Convolutional Layer\n",
        "- Pooling layer\n",
        "- CNN Architectures"
      ],
      "metadata": {
        "id": "RJCTMMDtSYZj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Image Data\n",
        "Upto now we have look into images that have 1 or 2 dimensions. Now, we are about to deal with the images that is usual made up to 3 dimensions. The 3 dimensions are as follows:\n",
        "- image height\n",
        "- image width\n",
        "- color channels\n",
        "\n",
        "    Color channels represents the depth of an images and coorelates to the colors used in it.\n",
        "\n",
        "    *For examples, an image with three channels lis likely made up of rgb(reb ,green, blue) pixels.*"
      ],
      "metadata": {
        "id": "35qoKseKTV_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolutional Neural Network\n",
        "\n",
        "Each convolutional neural network is made up of one or many convolutional layer. These layer are different from *dense* layer we have seen previously as there goal is to find the pattern from within the images that can be used to classify the images or parts of it.\n",
        "\n",
        "The fundamental difference between a dense layer and a convolutional layer is that dense layer detect pattern globally while convolutional layers detects pattern locally."
      ],
      "metadata": {
        "id": "0QjvWYZ7UtW2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating a Convnet\n",
        "\n",
        "It is bases on guide from Tensorflow documentation : https://www.tensorflow.org/tutorials/images/cnn"
      ],
      "metadata": {
        "id": "TbviL7LlYbvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing the necessary package"
      ],
      "metadata": {
        "id": "r2AuS1tadMwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "A9VfWn9iSZ63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "The problem we will consider here is classifying 10 different everyday object. The dataset we will use is built into tensorflow and called CIFAR Image Dataset. It contains 60,000 32 * 32 color images with 6000 images of each class.\n",
        "\n",
        "The labels in this dataset are as follows:\n",
        "\n",
        "- Airplane\n",
        "- Automobile\n",
        "- Bird\n",
        "- Cat\n",
        "- Deer\n",
        "- Dog\n",
        "- Frog\n",
        "- Horse\n",
        "- Ship\n",
        "- Truck\n",
        "\n",
        "Now, we will be loading the dataset"
      ],
      "metadata": {
        "id": "fW8bMReiYs27"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the datasets"
      ],
      "metadata": {
        "id": "D-eEBiKWdIpA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and split the datasets\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images/255.0 , test_images /255.0\n",
        "\n",
        "class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
      ],
      "metadata": {
        "id": "TsUKt7tncgCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's look at one image\n",
        "plt.imshow(train_images[1],cmap=plt.cm.binary)\n",
        "plt.xlabel(class_names[train_labels[1][0]])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "l1yruMtmdEGk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CNN Architecture\n",
        "\n",
        "A common architecture for a CNN is a stack of Conv2D and MaxPooling2D layers followed by a few densely connected layers. The idea is that the stack of convolutional and maxPooling layers extract the features from the image. Then these features are flattened and fed to densly connected layers that determine the class of an image based on the presence of features.\n",
        "\n",
        "Let's start by building the **Convolution Base**"
      ],
      "metadata": {
        "id": "np6Er7k8jx0b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.Sequential()\n",
        "model.add(layers.Conv2D(32,(3,3),activation='relu', input_shape=(32,32,3)))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2,2)))\n",
        "model.add(layers.Conv2D(64,(3,3),activation='relu'))"
      ],
      "metadata": {
        "id": "rmNenNjPdoML"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer 1**\n",
        "\n",
        "The input shape of our data will be 32, 32, 3 and we will process 32 filters of size 3 * 3 over our input data. We will also apply the activation function relu to the output of each convolution operation.\n",
        "\n",
        "**Layer 2**\n",
        "\n",
        "This layer perform the max pooling operation using 2*2 samples and a stride of 2\n",
        "\n",
        "**Other layer**\n",
        "\n",
        "The next set of layers do very similar things but take as input the features map from the previous layer. They also increase the frequency of filters from 32 to 64. We can do this as our data shrinks in spacial dimension as it passed through the layers, meaning we can afford(computationally) to add more depth."
      ],
      "metadata": {
        "id": "iKm6bBWjlAJt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "bsY-ZT7Zy8VW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Adding Dense Layers\n",
        "\n",
        "Upto now we have completed the convolution base. Now, we need to take these extracted features and add a way to classify them. This is why we add the following layers to the model."
      ],
      "metadata": {
        "id": "5RQEIJNdzPm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(64,activation='relu'))\n",
        "model.add(layers.Dense(10))"
      ],
      "metadata": {
        "id": "S7Tdleeey9Te"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "PY4aLTF2z44t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that the flatten layer changes the shape of our data so that we can feed it to the 64 node dense layer, followed by the final output layer of 10 neurons(one for each class)."
      ],
      "metadata": {
        "id": "G-0rzuC01j_V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "Now we will train and compile the model using the recommended hyper parameters from tensorflow.\n",
        "\n",
        "*Note: It might take longer time than other model*\n"
      ],
      "metadata": {
        "id": "k_xM3qxo26aW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])\n",
        "history = model.fit(train_images, train_labels, epochs=10,\n",
        "                    validation_data = (test_images, test_labels)\n",
        "                    )"
      ],
      "metadata": {
        "id": "KLCWrw81246p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model\n",
        "\n",
        "We can determine how well the model performed by looking at it's performance on the test data set."
      ],
      "metadata": {
        "id": "vRj-Rrlp6sX6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\n",
        "\n",
        "print(test_acc)"
      ],
      "metadata": {
        "id": "h4-m8XU23fB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Working with Small Datasets\n",
        "\n",
        "In the situation where you don't have millions of images, it is difficult to train a CNN from scratch that perform very well. This is why we will learn a few techniques we can use to train CNN on small datasets of just a few thousand images."
      ],
      "metadata": {
        "id": "8Hl35AI87PAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Augmentation\n",
        "\n",
        "Data augmentation is a technique to avoid over fitting and create a larger dataset from a smaller one.\n",
        "\n",
        "This is simply performing random transformation on our images so that our models can generalize better. These transformations can be things like compressions, rotations, stretches and even color changes.\n",
        "\n",
        "Let's look at the code below to an example of data augumentation."
      ],
      "metadata": {
        "id": "kY951HDH74lh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "\n",
        "# Creates a data generator object that transforms images\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=40,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Pick a image to transform\n",
        "\n",
        "test_img = train_images[14]\n",
        "img = img_to_array(test_img) # convert image into numpy array\n",
        "img = img.reshape((1,)+img.shape) # reshape image\n",
        "\n",
        "i = 0\n",
        "\n",
        "for batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):\n",
        "    plt.figure(i)\n",
        "    plot = plt.imshow(img_to_array(batch[0]))\n",
        "    i+=1\n",
        "    if i>4 :\n",
        "        break\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WtBIBEB96_wI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrained Models"
      ],
      "metadata": {
        "id": "MnTkEAhy-gA9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine Tuning"
      ],
      "metadata": {
        "id": "mYAdVBR--5nt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using a Pretrained Model"
      ],
      "metadata": {
        "id": "FPmoDJdu_Ens"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "keras = tf.keras"
      ],
      "metadata": {
        "id": "-AfVObDw-iTb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset\n",
        "\n",
        "We will load the *cats_vs_dogs* dataset from the module tensorflow_datasets."
      ],
      "metadata": {
        "id": "THs9qAqo_Xvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "# Split the data manually into 80% training, 10% cross validation, 10% validation\n",
        "(raw_train, raw_validation, raw_test), metadata = tfds.load(\n",
        "    'cats_vs_dogs',\n",
        "    split=['train[:80%]','train[80%:90%]','train[90%:]'],\n",
        "    with_info = True,\n",
        "    as_supervised = True,\n",
        ")"
      ],
      "metadata": {
        "id": "aWGWjOGR_WeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_label_name = metadata.features['label'].int2str #create a function object that we can use to get labels\n",
        "\n",
        "# display 2 images from the dataset\n",
        "for image, label in raw_train.take(2):\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.title(get_label_name(label))"
      ],
      "metadata": {
        "id": "pGDv74U6_vYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data Preprocessing\n",
        "\n",
        "Since the sizes of our images are all different we need to convert them all to the same size. We can create a function that will do that for us below."
      ],
      "metadata": {
        "id": "xZcDI0vgBCPU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE = 160 # All images will be resize to 160 * 160\n",
        "\n",
        "def format_example(image, label):\n",
        "    \"\"\"\n",
        "    returns an image that is reshape to IMG_SIZE\n",
        "    \"\"\"\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image/127.5) -1\n",
        "    image = tf.image.resize(image, (IMG_SIZE,IMG_SIZE))\n",
        "    return image, label"
      ],
      "metadata": {
        "id": "D2BLfc21BQvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can apply this function to all our images using map."
      ],
      "metadata": {
        "id": "5LniBS67Bswh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = raw_train.map(format_example)\n",
        "validation = raw_validation.map(format_example)\n",
        "test = raw_test.map(format_example)"
      ],
      "metadata": {
        "id": "1QfxmmekBrh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at our image."
      ],
      "metadata": {
        "id": "UxF9vNOsCPSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for image, label in train.take(2):\n",
        "    plt.figure()\n",
        "    plt.imshow(image)\n",
        "    plt.title(get_label_name(label))"
      ],
      "metadata": {
        "id": "4UzAy9cQB2NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we will shuffle and batch the images."
      ],
      "metadata": {
        "id": "I9o3CE3HCkFp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 32\n",
        "SHUFFLE_BUFFER_SIZE = 1000\n",
        "\n",
        "train_batches = train .shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "validation_batches = validation.batch(BATCH_SIZE)\n",
        "test_batches  = test.batch(BATCH_SIZE)"
      ],
      "metadata": {
        "id": "2g-8dy-ECtF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Picking a Pretrained Model\n",
        "\n",
        "The model we are going to use as the convolutional base for our model is the **MobileNet V2** developer by Google."
      ],
      "metadata": {
        "id": "W-XkGyeUDQ8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SHAPE = (IMG_SIZE,IMG_SIZE,3)\n",
        "\n",
        "# Create the base model from the pre-trained model MobileNet v2\n",
        "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
        "                                               include_top=False,\n",
        "                                               weights='imagenet')"
      ],
      "metadata": {
        "id": "67fvbDGRDObA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "MziJ4KKSD_rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image, _ in train_batches.take(1):\n",
        "    pass\n",
        "\n",
        "feature_batch = base_model(image)\n",
        "print(feature_batch.shape)"
      ],
      "metadata": {
        "id": "_vsrC5cqEESD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point the base model output a shape(32,5,5,1280) tensor that is a feature extration from our orginal (1,160,160,3) image. The 32 means that we have 32 layers of different filters."
      ],
      "metadata": {
        "id": "BlgK0r2nEwb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Freezing the Base\n",
        "\n",
        "The term **freezing** refers to disabling the training property of a layer. It simply means we wont make any changes to the weights of any layers that are frozen during training. This is important as we don't want to change the convolutional base that already has learned weights."
      ],
      "metadata": {
        "id": "PuURgnM-FGTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = False"
      ],
      "metadata": {
        "id": "-0Ife7swEoDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.summary()"
      ],
      "metadata": {
        "id": "BNAP3SQWFg2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Adding our Classifier\n",
        "\n",
        "Now that we have our base layer setup we can add the classifier. Instead of flattening the feature map of the base layer we will use a global average pooling layer that will average the entire 5*5 area of each 2D feature map and return to us a single 1280 element vector per filer.\n",
        "\n"
      ],
      "metadata": {
        "id": "ObkHddYyF3bF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()"
      ],
      "metadata": {
        "id": "vM_IpPkCFi3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally we will add the prediction layer that will be a single dense neuron.\n",
        "\n",
        "We can do this because we only have two classes to predict for."
      ],
      "metadata": {
        "id": "ncbLDPFlGa-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prediction_layer = keras.layers.Dense(1)"
      ],
      "metadata": {
        "id": "4of5kb1pGaJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we will combine these layers together in a model"
      ],
      "metadata": {
        "id": "pMiS6XbeGwzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    base_model,\n",
        "    global_average_layer,\n",
        "    prediction_layer\n",
        "])"
      ],
      "metadata": {
        "id": "rzrt-AqmGvvq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "k9wUe9LVHBv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model"
      ],
      "metadata": {
        "id": "mnDzhR5eHLop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_learning_rate = 0.0001\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate),\n",
        "              loss = tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R1p8YwXyHFbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epochs = 3\n",
        "validation_steps = 20\n",
        "\n",
        "loss0, accurac0 = model.evaluate(validation_batches, steps= validation_steps)"
      ],
      "metadata": {
        "id": "Yaw647V6HiEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can train it on our images\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=initial_epochs,\n",
        "                    validation_data=validation_batches)\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "print(acc)"
      ],
      "metadata": {
        "id": "4W7hLpBYH0wJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, saving and loading the model we have train"
      ],
      "metadata": {
        "id": "ut3Osjk2MkqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"dogs_vs_cats.h5\")\n",
        "new_model = tf.keras.models.load_model('dogs_vs_cats.h5')"
      ],
      "metadata": {
        "id": "u0Zdv1kgIVku"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}