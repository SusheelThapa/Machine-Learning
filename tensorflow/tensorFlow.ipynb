{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOsjRr8ptLRkeQPbmv9Prji",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SusheelThapa/ML-From-Scratch/blob/tensorflow/tensorflow/tensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning Fundamentals"
      ],
      "metadata": {
        "id": "vXmHX_AoHHFY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction of Tensorflow"
      ],
      "metadata": {
        "id": "hFfVDzrKKWOp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing Tensorflow\n",
        "\n",
        "Use the below command, to install the ***tensorflow** in your local machine\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```"
      ],
      "metadata": {
        "id": "kSAEmyFcLcg5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Tensorflow"
      ],
      "metadata": {
        "id": "XXv2op0hMHqF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.version"
      ],
      "metadata": {
        "id": "UVd0AIxhIqkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is tensor?\n",
        "\n",
        "Tensor is a generalization of vectors and matrices to potentially higher dimension.\n",
        "\n",
        "Internally, tensorflow represent tensors as  n-dimensional arrays of base datatypes.\n",
        "\n",
        "Each tensor has a data type and a shape\n",
        "\n",
        "**Data Types** includes: float32, int32, string and others\n",
        "\n",
        "**Shape**: Represents the dimension of data"
      ],
      "metadata": {
        "id": "Qj9it6lkNBdw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Creating tensor\n",
        "\n",
        "Below are the examples of creating tensor"
      ],
      "metadata": {
        "id": "_urOGnv5NIWP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "string = tf.Variable(\"This is a string\", tf.string)\n",
        "number = tf.Variable(324, tf.int16)\n",
        "floating = tf.Variable(3.567,tf.float64)"
      ],
      "metadata": {
        "id": "YYzKbmsjND7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rank/Degree of Tensors\n",
        "\n",
        "Another word for rank is degree, it can be define as the number of dimensions involved in the tensor.\n",
        "\n",
        "In the above code block, what we have created is *tensor of rank zero*\n",
        "\n",
        "Now, let's create tensor of higher degree/ranks"
      ],
      "metadata": {
        "id": "keBvVsfINf-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank1_tensor = tf.Variable([\"Something\",\"Nothing\"], tf.string)"
      ],
      "metadata": {
        "id": "lG0c8vtKNco-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To find the rank of the tensor we can call `rank()` method as "
      ],
      "metadata": {
        "id": "_AtY_iYSOKeo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.rank(rank1_tensor)"
      ],
      "metadata": {
        "id": "ajlKBwOZOJp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Shape of Tensors\n",
        "\n",
        "Shape of the tensors is simply the amount of elements that exist in each dimension.\n",
        "\n",
        "*Tensorflow will try to determine the shape of a tensor but sometimes it may be unknown*\n",
        "\n",
        "To get the shape of the tensor, we can call **shape attribute***"
      ],
      "metadata": {
        "id": "8uWPgHFdO__q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rank1_tensor.shape"
      ],
      "metadata": {
        "id": "gnRm5RYoOZhC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Changing the shape\n",
        "\n",
        "Number of elements of a tensor is the product of the sizes of all its shape.\n",
        "\n",
        "Due to which many shapes that have the same number of elements, making it convient to be able to change the shape of a tensor\n",
        "\n",
        "Example of changing the shape of tensor"
      ],
      "metadata": {
        "id": "XFRf8Dg8Pt4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensor1 = tf.ones([1,2,3]) # tf.ones will create tensor of provide shape will all its element of ones\n",
        "\n",
        "tensor2 = tf.reshape(tensor1,[3,2,1]) # reshape the existing tensor to shape [3,2,1]\n",
        "\n",
        "tensor3= tf.reshape(tensor2,[3,-1]) # -1 tells tensor to calculate the size of the dimension at that place\n",
        "\n",
        "# The number of elements in orginal tensor and the reshape tensor is same"
      ],
      "metadata": {
        "id": "WhEKqca6PfhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, lets have a look at the shape of the tensor we have created"
      ],
      "metadata": {
        "id": "NVLkqzKqRO91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tensor1.shape)\n",
        "print(tensor2.shape)\n",
        "print(tensor3.shape)"
      ],
      "metadata": {
        "id": "dH7HrQIxROTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Types of tensor\n",
        "\n",
        "Commonly used tensor are as follows:\n",
        "- Variable\n",
        "- Constant\n",
        "- Placeholder\n",
        "- SparseTensor"
      ],
      "metadata": {
        "id": "h_k8EN-9ReR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Core Learning Algorithms\n",
        "\n",
        "We will be studying 4 fundamental machine learning algorithms.\n",
        "\n",
        "- Linear Regression\n",
        "- Classification\n",
        "- Clustering\n",
        "- Hidden Markov Models\n"
      ],
      "metadata": {
        "id": "Js7sza0dWmXp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear Regression\n",
        "\n",
        "Linear regression is a linear approach for modelling the relationship between a scalar response and one or more explanatory variables (also known as dependent and independent variables). (***Wikipedia***)"
      ],
      "metadata": {
        "id": "uFXk6GTlXRpO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Setup and Imports"
      ],
      "metadata": {
        "id": "5oGWz7X_YE_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import numpy as np # Optimize version of array\n",
        "import pandas as pd # Data analytics tools\n",
        "import matplotlib.pyplot as plt # Visualization tools\n",
        "\n",
        "import IPython.display as clear_output\n",
        "from six.moves import urllib\n",
        "\n",
        "import tensorflow.compat.v2.feature_column as fc # Required later in linear regression\n",
        "\n",
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "NQPdzKz8Wxvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data\n",
        "\n",
        "The dataset we will be focusing here will be titanic dataset. It has tons of information about each passanger on the ship.\n",
        "\n",
        "**Below, we will load a dataset and learn how we can explore it using some built-in tools**"
      ],
      "metadata": {
        "id": "rByhX4GVZYZW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "\n",
        "# Training datasets\n",
        "dftrain = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv') \n",
        "y_train = dftrain.pop('survived')\n",
        "\n",
        "# Testing datasets\n",
        "dftest = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv') # Testing datasets\n",
        "y_test = dftest.pop('survived')"
      ],
      "metadata": {
        "id": "qeP_g3AZYi9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`pd.read_csv()` method will return a new pandas *dataframe*. Dataframe is like a table and actually have a look at the table representation.\n",
        "\n",
        "We have decided to pop the \"survived\" column from our dataset and store it in a new varible as this column tells us whether the passanger survived or not. It is most like to be something that our model should predict\n",
        "\n",
        "To look at the data we will use `head()` method from pandas."
      ],
      "metadata": {
        "id": "m58q78WMaak7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.head()"
      ],
      "metadata": {
        "id": "i4rinQLQZvA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And if we need more statical description of the data we can use `describe()` method"
      ],
      "metadata": {
        "id": "HoltfuBkcOj3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.describe()"
      ],
      "metadata": {
        "id": "1d95wBQZbMES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the information about the dataype of each column, number of columns and what are those we can use `info()` method of pandas"
      ],
      "metadata": {
        "id": "Ec-wPrmHcnqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.info()"
      ],
      "metadata": {
        "id": "Wn5BhkLgceTK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the shape of the dataframe"
      ],
      "metadata": {
        "id": "NA8l_30xdFYC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.shape"
      ],
      "metadata": {
        "id": "ibSwsp_-cmQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the data we have got."
      ],
      "metadata": {
        "id": "SD2gG2zZdZkz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.age.hist(bins=20)"
      ],
      "metadata": {
        "id": "q3vCndn5dL5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain.sex.value_counts().plot(kind='barh')"
      ],
      "metadata": {
        "id": "7tumauZzdgDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dftrain['class'].value_counts().plot(kind='barh')"
      ],
      "metadata": {
        "id": "h06xPqUtdmrZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.concat([dftrain,y_train],axis=1).groupby('sex').survived.mean().plot(kind='barh').set_xlabel('% survived')"
      ],
      "metadata": {
        "id": "5JLOOC3jueKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After analyzing this information we should notice the following:\n",
        "- The majority of passangers are in their 20's or 30's\n",
        "- The majority of passengers are male\n",
        "- The majority of passengers are in \"Third Class\"\n",
        "- Females have a much higher chances of survival"
      ],
      "metadata": {
        "id": "8_yLgfv2d__B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training vs Testing Data\n",
        "\n",
        "**Training Data** is what we feed to the model so that it can develop and learn. It is usually much larger size than the testing data\n",
        "\n",
        "**Testing Data** is what we use to evaluate the model and see how well it is performing. It is important to use seperate set of data that the model has not been trained on to evaluate it.\n"
      ],
      "metadata": {
        "id": "fa16QYvdetm4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Features Columns\n",
        "\n",
        "In the dataset, we have two types of information\n",
        "- **Categorical**\n",
        "\n",
        "    It is anything that isn't numerical.\n",
        "\n",
        "    *For example, the sex column does use numbers, it use words 'male' and \"female\"*\n",
        "\n",
        "- **Numeric**\n",
        "\n",
        "    These are the data with numeric value.\n",
        "\n",
        "Before continuing, we need to change all our categorical data into numeric data.\n",
        "Todo this, Tensorflow has some tools to help us."
      ],
      "metadata": {
        "id": "Ub4n1pT2lq3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CATEGORICAL_COLUMNS = ['sex','n_siblings_spouses','parch','class','deck','embark_town','alone']\n",
        "NUMERIC_COLUMNS = ['age','fare']\n",
        "\n",
        "feature_columns =[]\n",
        "\n",
        "for feature in CATEGORICAL_COLUMNS:\n",
        "    vocabulary = dftrain[feature].unique()\n",
        "    feature_columns.append(tf.feature_column.categorical_column_with_vocabulary_list(feature,vocabulary))\n",
        "\n",
        "\n",
        "for feature in NUMERIC_COLUMNS:\n",
        "    feature_columns.append(tf.feature_column.numeric_column(feature,dtype=tf.float32))\n"
      ],
      "metadata": {
        "id": "mfHMpFK1dsCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model\n",
        "\n",
        "Training the model describes about how the model is being train. Specifically speaking how data is fed to our model.\n",
        "\n",
        "To train the model, we will fed the model with data of batch size of 32. It means we will fed small batches of entries to our model multiple times according to the **epoches**\n",
        "\n",
        "**Epoches** is one stream of our entire datasets. Number of epoches we define is the amount of times our model will see the entire dataset.\n",
        "\n",
        "*Examples: If we have 10 epocs, our model will see the same datasets 10 times.*\n",
        "\n",
        "To feed our data to model in the form of batches we need ***input function*** which task is to convert our dataset into batches at each epoch"
      ],
      "metadata": {
        "id": "2R_RzjXIvrK9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Input function\n",
        "\n",
        "The Tensorflow model we are going to use requires that the data we pass it comes in as `tf.data.Dataset` object.\n",
        "\n",
        "It means that we must create a *input function* that can convert our current pandas dataframe into that object.\n",
        "\n",
        "*input_function* show below is directly copied from tensorflow documentation."
      ],
      "metadata": {
        "id": "lNv3gy-vxHTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_input_fn(data_df, label_df, num_epochs=10, shuffle=True, batch_size=32):\n",
        "  def input_function():\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dict(data_df), label_df)) # Create tf.data.Dataset object with data and its label\n",
        "    if shuffle:\n",
        "      ds = ds.shuffle(1000) # randomize the order of data\n",
        "    ds = ds.batch(batch_size).repeat(num_epochs) # split dataset into batches of 32 and repeat the process for number of epochs\n",
        "    return ds # return a batch of the dataset\n",
        "  return input_function # return a function object for use\n",
        "\n",
        "train_input_fn = make_input_fn(dftrain, y_train)\n",
        "eval_input_fn = make_input_fn(dftrain, y_train, num_epochs=1, shuffle=False)"
      ],
      "metadata": {
        "id": "RIQp6anExEL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Creating the model\n",
        "\n",
        "We will be using linear estimator to utilize the linear regression algorithm."
      ],
      "metadata": {
        "id": "-9ubaazCzlOn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns) # We are creating a linear estimator by passing the feature columns we created earlier."
      ],
      "metadata": {
        "id": "h89lu4Diz0MT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Training the model\n",
        "\n",
        "Training the model is as easy as passing the input functions that we created earlier."
      ],
      "metadata": {
        "id": "SmWbzVNV0jGd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the model\n",
        "linear_est.train(train_input_fn) # just passing the input function"
      ],
      "metadata": {
        "id": "aL_MWepPz8if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing our model\n",
        "Testing is also same as training the model but here we will be passing input function for testing dataset"
      ],
      "metadata": {
        "id": "0XKkMNxD1Lc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = linear_est.evaluate(eval_input_fn)\n",
        "\n",
        "print(\"The accuracy of our model is \",result['accuracy'])"
      ],
      "metadata": {
        "id": "CWggySSZ1XAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Predicting using our model"
      ],
      "metadata": {
        "id": "CMYdP9Cl2uVh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = list(linear_est.predict(eval_input_fn))\n",
        "\n",
        "print(\"Passanger chance of survival is \",result[100]['probabilities'][1])"
      ],
      "metadata": {
        "id": "-fV0HcKG2wj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classification"
      ],
      "metadata": {
        "id": "hVDXxsS3ATy-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Importing the necessary packages"
      ],
      "metadata": {
        "id": "RptsNiM7BuJi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "WBpn50ubB0HA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Datasets\n",
        "\n",
        "This species dataset seperates the flower into 3 different classes of species\n",
        "- Setosa\n",
        "- Versicolor\n",
        "- Virginica\n",
        "\n",
        "The information about each flower is the following:\n",
        "- sepal length\n",
        "- sepal width\n",
        "- petal length\n",
        "- petal width\n"
      ],
      "metadata": {
        "id": "a8Cq2Gv5BFta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loading the datasets\n",
        "\n",
        "Next, we will be loading the datasets"
      ],
      "metadata": {
        "id": "nod3RD7RBn32"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining some constant that will help later on\n",
        "CSV_COLUMN_NAMES = [\"SepalLength\",\"SepalWidth\",\"PetalLength\",\"PetalWidth\",\"Species\"]\n",
        "SPECIES = [\"Setosa\",\"Versicolor\",\"Virginica\"]\n",
        "\n",
        "# Loading the datasets, we are using keras to grab our datasets and read them into pandas dataframe\n",
        "train_path = tf.keras.utils.get_file(\n",
        "    \"iris_training.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_training.csv\"\n",
        ")\n",
        "test_path = tf.keras.utils.get_file(\n",
        "    \"iris_test.csv\",\"https://storage.googleapis.com/download.tensorflow.org/data/iris_test.csv\"\n",
        ")\n",
        "\n",
        "train = pd.read_csv(train_path,names= CSV_COLUMN_NAMES,header =0)\n",
        "test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)"
      ],
      "metadata": {
        "id": "688qXySJAWcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at our datasets."
      ],
      "metadata": {
        "id": "DsNEN21tDTLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.head()"
      ],
      "metadata": {
        "id": "e3tusWKjCqye"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can pop the \"Species\" as they are label to classify."
      ],
      "metadata": {
        "id": "4-HW2xH9DZB8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = train.pop('Species')\n",
        "y_test = test.pop('Species')"
      ],
      "metadata": {
        "id": "eXQLXQfRC1QA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look into shape of our datasets"
      ],
      "metadata": {
        "id": "DYf4Du8XEL6B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train.shape"
      ],
      "metadata": {
        "id": "w3L8YA7YDjB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, we have 120 data with 4 features"
      ],
      "metadata": {
        "id": "eDCllXq3ETa2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Input function"
      ],
      "metadata": {
        "id": "_FZoaDwTEekr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn(features, labels, training=True, batch_size=256):\n",
        "    # Convert the inputs to a dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((dict(features),labels))\n",
        "\n",
        "    # Shuffle and repeat if you are in training mode\n",
        "    if training:\n",
        "        dataset= dataset.shuffle(1000).repeat()\n",
        "    \n",
        "    return dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "mXDuwhHnEgto"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Features columns"
      ],
      "metadata": {
        "id": "N8i_tlQoFQWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "my_feature_columns = []\n",
        "\n",
        "for key in train.keys():\n",
        "    my_feature_columns.append(tf.feature_column.numeric_column(key=key))"
      ],
      "metadata": {
        "id": "dammCcgPFNRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Building the model\n",
        "\n",
        "For classification tasks there are variety of different esitmators/models that we can pick from.\n",
        "\n",
        "Some options are listed below:\n",
        "\n",
        "- `DNNClassifier`(Deep Neural Network)\n",
        "- `LinearClassifier`\n",
        "\n",
        "We can choose either model but DNN is the best choice as we may not be able to  find a linear correspondence in our data."
      ],
      "metadata": {
        "id": "YoBayCnyFs9V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Building a DNN with 2 hidden layer with 30 and 10 hidden nodes each\n",
        "classifier = tf.estimator.DNNClassifier(\n",
        "    feature_columns = my_feature_columns,\n",
        "    hidden_units=[30,10], # Defining the two hidden layer\n",
        "    n_classes =3 #model must chose between 3 classes\n",
        ")"
      ],
      "metadata": {
        "id": "SCJyOyztFiGB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training the model"
      ],
      "metadata": {
        "id": "mRiM8HT3HSVw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.train(\n",
        "    input_fn = lambda: input_fn(train,y_train, training=True),\n",
        "    steps=5000\n",
        ")"
      ],
      "metadata": {
        "id": "U63pOGwtHVvZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing the model"
      ],
      "metadata": {
        "id": "5KM7dfq7IJHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.evaluate(\n",
        "    input_fn=lambda:input_fn(test, y_test, training = False)\n",
        "    )"
      ],
      "metadata": {
        "id": "bAYEgc0EILbl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Making prediction"
      ],
      "metadata": {
        "id": "YIFlosljI2BF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def input_fn_pred(features, batch_size=256):\n",
        "    # Convert the inputs to a Dataset without labels.\n",
        "    return tf.data.Dataset.from_tensor_slices(dict(features)).batch(batch_size)\n",
        "\n",
        "features = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n",
        "predict = {}\n",
        "\n",
        "print(\"Please type numeric values as prompted.\")\n",
        "for feature in features:\n",
        "  valid = True\n",
        "  while valid: \n",
        "    val = input(feature + \": \")\n",
        "    if not val.isdigit(): valid = False\n",
        "\n",
        "  predict[feature] = [float(val)]\n",
        "\n",
        "predictions = classifier.predict(input_fn=lambda: input_fn_pred(predict))\n",
        "for pred_dict in predictions:\n",
        "    class_id = pred_dict['class_ids'][0]\n",
        "    probability = pred_dict['probabilities'][class_id]\n",
        "\n",
        "    print('Prediction is \"{}\" ({:.1f}%)'.format(\n",
        "        SPECIES[class_id], 100 * probability))\n"
      ],
      "metadata": {
        "id": "9SCEHkktIfbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering\n",
        "\n",
        "Clustering is a machine learning technique that involves the grouping of data points. In theory, data points that are in the same group should have similar properties and/or features, while data points in different groups should have highly dissimilar properties and/or features."
      ],
      "metadata": {
        "id": "LUB2XE4wLV6v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Algorithm for K-means clustering\n",
        "\n",
        "- Randomly pick K points to place K centroids\n",
        "- Assign all of the data points to the centroids by distance. The closest centroid to a point is the one it is assigned to.\n",
        "- Average all of the points belonging to each centroid to find the middle of those clusters(center of mass). Place the corresponding centroids into that position\n",
        "- Reassign every point once again to the closest centroid\n",
        "- Repeat steps 3-4 until no points changes which centroid it belongs to"
      ],
      "metadata": {
        "id": "YM_4vcAgP4kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hidden Markov Models\n",
        "\n",
        "Hidden Markov Models is a finite sets of state, each of which is associated with a(generally multidimensional) probality distribution.\n",
        "\n",
        "Transition among the states are governed by a set of **probalities** called transition probalities.\n",
        "\n",
        "A hidden markov models works with probalities to predict future events or states.\n",
        "\n",
        "We will be creating a hidden markov model that can predict the weather.\n"
      ],
      "metadata": {
        "id": "lRTdoF8gSgB_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data in Hidden Markov Models\n",
        "\n",
        "**States**\n",
        "\n",
        "In each markov model we have a finite set of states. These states could be something like \"warm\" and \"cold\" or \"high\" and \"low\".\n",
        "\n",
        "These states are \"hidden\" within the model, which mean we do not directly observe them.\n",
        "\n",
        "**Observations**\n",
        "\n",
        "Each state had a particular outcome or observation assocaited with it based on probality distribution.\n",
        "\n",
        "*Examples: On a hot day, Tim has a 80% chance of being happy and a 20% chance of being sad.*\n",
        "\n",
        "**Transitions**\n",
        "\n",
        "Each state will have a probability defining the likelyhood of transforming to a different state. \n",
        "\n",
        "*Example: A cold day has a 30% chance of being followed by a hot day and a 70% chance of being followed by another cold day*\n",
        "\n",
        "To create a hidden markov model we need:\n",
        "- States\n",
        "- Observation Distribution\n",
        "- Transition Distribution\n",
        "\n",
        "For our purpose, we will assume we already have this information avaliable as we attempt to predict the weather on a given day.\n",
        "\n"
      ],
      "metadata": {
        "id": "KYHiTnDLTTKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Networks\n",
        "\n",
        "An artificial neural network learning algorithm, or neural network, or just neural network, is a computational learning system that uses a network of functions to understand and translate a data input of one form into a desired output, usually in another form. The concept of the artificial neural network was inspired by human biology and the way neurons of the human brain function together to understand inputs from human senses. *(https://deepai.org/machine-learning-glossary-and-terms/neural-network)*"
      ],
      "metadata": {
        "id": "kvx_tNQk61_R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "-6U2mPWQ-rPT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "biV8oeW964HB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Datasets\n",
        "\n",
        "We will be using MNIST Fashain Dataset. This dataset is included in keras and it includes 60000 images for training and 10,000 images for validation/testing."
      ],
      "metadata": {
        "id": "fJjsOELN-4qT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset\n",
        "fashion_mnist = keras.datasets.fashion_mnist\n",
        "\n",
        "# Spliting into training and testing datasets\n",
        "(train_images, train_labels),(test_images, test_labels) = fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "D5hpTap3-33b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look as it shape"
      ],
      "metadata": {
        "id": "C1gTcA33_hnk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images.shape"
      ],
      "metadata": {
        "id": "_iLMHc-Q_QuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It means that we have got 60000 images of 28 * 28 pixels for training the models"
      ],
      "metadata": {
        "id": "kcLG3weL_t9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images[0,23,23]"
      ],
      "metadata": {
        "id": "gERIwBys_n9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our pixel values are between 0 and 255, 0 being black and 255 being white. It means that we have grayscale images as there are no color other than black and white."
      ],
      "metadata": {
        "id": "NCqcOioMATGn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels[:10]"
      ],
      "metadata": {
        "id": "Y98hSZJzAEKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our label ranges from 0-9. Each integer represents a specific article of clothing."
      ],
      "metadata": {
        "id": "70kXYB5CBI3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['T-shirt/top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']"
      ],
      "metadata": {
        "id": "DHULkAH4Akc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally lets have a look at what some of these images look like"
      ],
      "metadata": {
        "id": "3TSAt_rcBV9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "plt.imshow(train_images[1])\n",
        "plt.colorbar()\n",
        "plt.grid(False)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_dNr2_TKBUlW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing\n",
        "\n",
        "Data preprocessing means applying some prior transformation to our data before feeding to the model. In this case, we will simply scale all of our greyscale pixel values(0-255) to be between 0 and 1.\n",
        "\n",
        "To do this, we will divided training and testing sets by 255.0. We do this because smaller values will make it easier for the model to process our values."
      ],
      "metadata": {
        "id": "vEwRHVE-BokN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_images =  train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "metadata": {
        "id": "7FWlXdVpBquF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building the model\n",
        "\n",
        "We are going to use keras *sequential* model with three different layer. This model represents a feed-forward neural network(one that passes values from left to right)."
      ],
      "metadata": {
        "id": "0cMO-mfrCj4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28,28)), # input layer (1)\n",
        "    keras.layers.Dense(128,activation='relu'), # hidden layer (2)\n",
        "    keras.layers.Dense(10,activation='softmax')# output layer (3)\n",
        "])"
      ],
      "metadata": {
        "id": "DMSBcUnYCZIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Layer 1**\n",
        "\n",
        "This is our input layer and it will consist fo 784 neurons. We use the flatten layer with an input shape of (28,28) to denote that our input should come in that shape. The flatten means that our layer will reshape the shape (28,28) array into a vector of 784 neurons so that each pixel will be associated with one neuron.\n",
        "\n",
        "**Layer 2**\n",
        "\n",
        "This is our first and only hidden layer. The dense denotes that this layer will be fully connected and each neyron from the previous layer connects to each neuron of this layer. It has 128 neurons and uses the rectify linear unit activation function.\n",
        "\n",
        "**Layer 3**\n",
        "\n",
        "This is our output layer and is also a dense layer. It has 10 neurons that we will look at to determine our models output. Each neuron represents the probabillity of a given images being one of the 10 different classes. The activation function *softmax* is used on this layer to calculate a probabillity distribution for each class. This means the value of any neuron in this layer will be between 0 and 1, where 1 represents a high probability of the image being that class."
      ],
      "metadata": {
        "id": "xgeP3nk9EN7e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compile the Model\n",
        "\n",
        "This is the last step of building the model. While compiling the model we will specify the loss function, optimizer adn metrics we would like to track.\n",
        "\n",
        "We will be using ***sparse_categorical_crossentropy*** as loss function and ***adam*** as optimizer."
      ],
      "metadata": {
        "id": "vC5kP0EkFwFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "DYRCZdBFFt6y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the model\n",
        "\n",
        "Now that we have build the model, we will be training the model with the preprocessed data."
      ],
      "metadata": {
        "id": "szU6HjYZGk3c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(train_images,train_labels,epochs=5)"
      ],
      "metadata": {
        "id": "TE52gQmjGZkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluating the model\n",
        "\n",
        "After we have train the model, we will be looking into evaluating the model we have created.\n",
        "\n",
        "The *verbose* argument can have two values 0 or 1 where 0 = slient and 1 = progress bar."
      ],
      "metadata": {
        "id": "5hqAgCnMHe4J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=1)\n",
        "\n",
        "print(\"Test accuracy = \", test_acc)"
      ],
      "metadata": {
        "id": "JTB5B6_cHnP3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions\n",
        "\n",
        "To make predictions we simply need to pass an array of data in the form we've specified in the input layer to `.predict()` method."
      ],
      "metadata": {
        "id": "q3boEMc1IlRN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(test_images)"
      ],
      "metadata": {
        "id": "BQAzhI9sH-bv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "THis method returns to us an array of predictions for each images we passed to it. Let's have a look at the predictions for image 1."
      ],
      "metadata": {
        "id": "ug38jIU-JS_c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions[0]"
      ],
      "metadata": {
        "id": "tV_LaSPPI0Wg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we want to get the value withthe highest score we can use `argmax()` function from numpy. This simply returns hte index of the maximum value from an numpy array."
      ],
      "metadata": {
        "id": "uLd_qyWFKraY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "label_index = np.argmax(predictions[0])\n",
        "print(\"Prediction is \", class_names[test_labels[label_index]])"
      ],
      "metadata": {
        "id": "jNudG5XNJLXY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets check if the prediction is correct or not"
      ],
      "metadata": {
        "id": "TL4io9HKMcgC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(test_images[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GnrgdYr3MGkR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}